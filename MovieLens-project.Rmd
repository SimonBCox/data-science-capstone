---
title: "The Movie Lens project"
author: "Simon Cox"
date: "24-12-2019"
output:
  pdf_document: default
  html_document: default
---

```{r Create subsets, include=FALSE}
################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# Clean up memory by removing data sets which will not be used anymore
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
# Before we start
Dear reader, thank you for taking time to read my work. I know that if you also participate in the data science course, then it is not out of free will, but still. I tried to write it all down as structured as possible to make it easy for you to read, enjoy!

# Introduction
This document contains the Movie Lens project which is part of the **edX Data Science Professional Programm** capstone project. The goal of this project is to create a  movie recommendation system based on the MovieLens dataset. This recommendation system will consist of a trained machine learning algorithm. The quality of this recommendation system will be assessed by means of the residual mean squared error or RMSE. The deliverables for this project will be this R markdown document in .Rmd and .pdf format and the complete R script in .R format. All files are available in this github repository: https://github.com/SimonBCox/data-science-capstone.git

The MovieLens dataset is split in a training set (`edx`) and a validation set (`validation`). The `edx` dataset will be used to train the recommendation system and the `validation` set will be used to calculate the final RMSE. Both sets are generated using a script provided by the course instructors. This provided script will not be shown in this document.

# Method
Let us start with a short description of the `edx` and `validation` datasets. Both sets are of class `r class(edx)`. The `edx` set contains `r nrow(edx)` rows and `r ncol(edx)` columns. The table below shows the column names and the number of unique entries for each column of the `edx` set.

```{r echo=FALSE}
edx %>% summarise_all(list(~n_distinct(.))) %>% `rownames<-`(.,"unique entries")
```

The `validation` dataset is about 9 times smaller than the `edx` dataset and contains `r nrow(validation)` rows and also `r ncol(validation)` columns. Below the number of unique entries for each column are shown.

```{r echo=FALSE}
validation %>% summarise_all(list(~n_distinct(.))) %>% `rownames<-`(.,"unique entries")
```

On the next page the distributions for the movieId column and userId column are presented for both sets. The images show for each unique entry in the movieId column how many times it has been rated and for each unique entry in the userId column how many ratings it has given. We can see a different distribution for the moviesId's in the `edx` and `validation` set. The `validation` set seems to have more movieId's with just a few ratings. With regard to the usersId's both sets seem to have a similar distribution.

```{r fig.width = 10, fig.height=4, echo=FALSE}
p1 <- edx %>%
     dplyr::count(movieId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 20, color = "black") + 
     scale_x_log10() + 
     ggtitle("Movies - edx")

p2 <- validation %>%
     dplyr::count(movieId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 20, color = "black") + 
     scale_x_log10() + 
     ggtitle("Movies - validation")
grid.arrange(p1, p2, ncol=2)
```

```{r fig.width = 10, fig.height=4, echo=FALSE}
p1 <- edx %>%
     dplyr::count(userId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 20, color = "black") + 
     scale_x_log10() + 
     ggtitle("Users - edx") 

p2 <- validation %>%
     dplyr::count(userId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 20, color = "black") + 
     scale_x_log10() + 
     ggtitle("Users - validation") 
grid.arrange(p1, p2, ncol =2)
```

The `edx` dataset is very large which results in a memory error when using the `lm()` function. To avoid memory errors, the procedure and techniques are used as described in the data science book Chapter 33 Large datasets.

Similar to the approach in Chapter 33, the recommendation system will be build step-by-step. For this project the recommendation system wil be build in four consecutive steps:

1. Movie effect model
2. Movie and user effect model
3. Movie, user and genres effect model
4. Regularized movie, user and genre effect model

With each (sub)model we will calculate the RMSE and present it in order to show the difference each step makes.\
The RMSE is calculated using the `calculate_RMSE` function, which requires `true_ratings` and `predicted_ratings` as input.

```{r}
calculate_RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
\pagebreak

# 1. Movie effect model
Before we start including any effects we need to calculate the average rating (`mu`). This value of `mu` will be used in all four models.

``` {r}
mu <- mean(edx$rating)
mu
```

Now we can include the movie effect in the recommendation system by a movie specific bias (`b_i`). `b_i` is calculated for each unique movie as the average of each rating minus `mu` and is stored in `movie_avgs`. The distribution of `b_i` is substantial as shown in the figure below. 

``` {r}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```

```{r fig.width = 5, fig.height=4, fig.align="center",  echo=FALSE}
qplot(b_i, data = movie_avgs, bins = 15, color = I("black"))
```

Now the movie effect is known we can predict the ratings for the `validation` set. The predicted ratings will be stored in `predicted_ratings`, which server as input for the `calculate_RMSE` function. 

``` {r}
predicted_ratings <- validation %>%
  left_join(movie_avgs, by='movieId') %>% # add column b_i
  mutate(pred = mu + b_i) %>%
  pull(pred)

calculate_RMSE(predicted_ratings, validation$rating)
```

```{r include=FALSE}
options(pillar.sigfig = 4) # set number of significant digits to 6
summary <- tibble(Method = "Movie effect model", RMSE = calculate_RMSE(predicted_ratings, validation$rating))
```

As we can see the RMSE is higher then 0.9, which is too high. So, let us continue and improve the model.
\pagebreak

# 2. Movie and user effect model
In this second step the user effect is added to the recommendation system by means of a user specific bias (`b_u`). The user specific bias is calculated in a similar way as `b_i` and will be stored in `user_avgs`. The distribution of `b_u` is also substantial as shown in the figure below. 

```{r}
user_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%  # add column b_i
  group_by(userId) %>%                     
  summarize(b_u = mean(rating - mu - b_i))
```

```{r fig.width = 5, fig.height=4, fig.align="center",  echo=FALSE}
qplot(b_u, data = user_avgs, bins = 15, color = I("black"))
```

With the movie and user effects now known we can predict the ratings for the `validation` set and run the `calculate_RMSE` function.

``` {r}
predicted_ratings <- validation %>%
  left_join(movie_avgs, by='movieId') %>%  # add column b_i
  left_join(user_avgs, by='userId') %>%    # add column b_u
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

calculate_RMSE(predicted_ratings, validation$rating)
```

``` {r echo=FALSE}
summary <- add_row(summary, Method = "Movie and user effect model", RMSE = calculate_RMSE(predicted_ratings, validation$rating))
```

As we can see the RMSE decreases to below 0.9, which is good but we have two more steps that hopefully improve the model a bit more.
\pagebreak

# 3. Movie, user and genres effect model
In this thrid step we add the genres effect to the recommendation system by means of a genres specific bias (`b_g`). Again, the calculation procedure for `b_g` is similar to that of `b_i` and `b_u` and the values will be stored in `genres_avgs`. The distribution of `b_g` is less substantial than we have seen with `b_i` and `b_u`, see the figure below.

```{r}
genres_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%  # add column b_i
  left_join(user_avgs, by='userId') %>%    # add column b_u
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))
```

```{r fig.width = 5, fig.height=4, fig.align="center",  echo=FALSE}
qplot(b_g, data = genres_avgs, bins = 15, color = I("black"))
```

Now we have all the ingredients to predict the ratings for the `validation` set based on the movie, user and genre effects.

``` {r echo = FALSE}
predicted_ratings <- validation %>%
  left_join(movie_avgs, by='movieId') %>%  # add column b_i
  left_join(user_avgs, by='userId') %>%    # add column b_u
  left_join(genres_avgs, by='genres') %>%  # add column b_g
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)                                        

calculate_RMSE(predicted_ratings, validation$rating)
```

```{r echo=FALSE}
summary <- add_row(summary, Method = "Movie, user and genre effect model", RMSE = calculate_RMSE(predicted_ratings, validation$rating))
```

Again the RMSE decreases. Now below 0.865. Let us continue to the final step to see how low we can get.
\pagebreak

# 4. Regularized movie, user and genre effect model
In this final step the model is regularized with a penalty term ($\lambda$). This is done to make the model more robust and reduce the influence of outliers. To do this we first create the function `calculate_RMUGEM`, which takes $\lambda$ as input and calculates the RMSE using the regularized movie, user and gerne effects model or RMUGEM.

```{r}
calculate_RMUGEM <- function(l){
    b_i <- edx %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  
  b_g <- edx %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u)/(n()+l))
  
  predicted_ratings <- validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_g) %>%
    pull(pred)
  
  return(calculate_RMSE(predicted_ratings, validation$rating))
}
```

To determine the best value for $\lambda$ we create an array containing 50 values for $\lambda$ and strore them in `lambdas`.
```{r}
lambdas <- seq(2.5, 7.5, 0.1)
```

Then we use `sapply` to perform the `calculate_RMUGEM` for each $\lambda$. The resulting RMSE's are stored in `RMSEs`. 
```{r}
RMSEs <- sapply(lambdas, calculate_RMUGEM)
```

We can plot the RMSE's against the lambdas and see that a lambda of $\lambda$ = `r lambdas[which.min(RMSEs)]` provides the minimum RMSE of `r min(RMSEs)`.

```{r fig.width = 5, fig.height=2, fig.align="center", echo=FALSE}
options(digits=5)
qplot(lambdas, RMSEs)  
```

```{r echo=FALSE}
summary <- add_row(summary, Method = "Regularized movie, user and genre effect model", RMSE = min(RMSEs))
```

\pagebreak

# Results
In the table below the RMSE's for all four consecutive models are presented. It shows that for each effect that we add the RMSE reduces. And finally, an even smaller RMSE is reached by regularizing the effects. A benefit of this approach is that the calculation time for this model is rather low. Downloading the MovieLens dataset almost consumes the most amount of time.

```{r echo = FALSE}
options(digits=5)
summary %>% knitr::kable()
```

# Conclusion
A movie recommendation model was created based on the MovieLens dataset. The final model incorporates regularized movie, user and genre effects and is trained with the `edx` dataset. The quality of the model is assessed by predicting ratings for the `validation` set and calculating the RMSE. With the final model a RMSE of `r min(RMSEs)` is achieved.

Although the model is built step-by-step, it only uses one type of machine learning algorithm. This is mainly due to the fact that the `edx` dataset was that large that using the `train()` function resulted in a memory error. A smaller partition of the `edx` dataset has been created to enable the use of the `train()` function. However, the calculation times was still disproportionately high in comparison to the results. Therefore, this path has been abandoned.

Concerning future work, I don't think I will continue working on the MovieLens dataset. But I will definetly continue working on other datasets which are more related to my field of expertice, which is structural engineering.

# Concluding remark
Thank you for reading this report. I hope you liked it. 
Have a nice day!

Kind regards,

Simon Cox

